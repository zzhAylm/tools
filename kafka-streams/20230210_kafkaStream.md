```
TimestampExtractor: 时间戳

Stream as Table：流可以被认为是表的变更日志，其中流中的每个数据记录都捕获表的状态更改。因此，流是一个伪装的表，通过从头到尾重放更改日志以重建表，它可以很容易地变成一个“真正的”表。类似地，在更一般的类比中，在流中聚合数据记录——例如从页面浏览事件流中计算用户的页面浏览总数——将返回一个表（这里的键和值是用户及其对应的浏览量，分别）。
Table as Stream：表可以被认为是流中每个键的最新值在某个时间点的快照（流的数据记录是键值对）。因此，一个表是一个伪装的流，通过遍历表中的每个键值条目，它可以很容易地变成一个“真正的”流。

```



### 聚合：

```
聚合操作采用 一个输入流或表，并通过将多个输入记录组合成一个输出记录来生成一个新表。聚合的示例是计算计数或总和。

在 中Kafka Streams DSL，一个输入流aggregation可以是一个 KStream 或一个 KTable，但输出流总是一个 KTable。这允许 Kafka Streams 在值生成和发出后，在更多记录无序到达时更新聚合值。当发生这种乱序到达时，聚合 KStream 或 KTable 发出一个新的聚合值。因为输出的是一个KTable，所以在后续的处理步骤中，新值被认为是用相同的key覆盖了旧值。


将多个输入记录组合成一个输出记录，来生成一个新的表，聚合后输入的总是一个KTable

分组也是聚合的一种，aggregation聚合，将建相同的数据聚合在一起，进行操作

```

### 容错：

```
Kafka Streams 建立在 Kafka 中本地集成的容错功能之上。Kafka 分区是高可用和可复制的；因此，当流数据持久化到 Kafka 时，即使应用程序失败并需要重新处理它也是可用的。Kafka Streams 中的任务利用 Kafka 消费者客户端提供的容错功能来处理故障。如果任务在失败的机器上运行，Kafka Streams 会自动在应用程序的剩余运行实例之一中重新启动任务。
```

### 时间：

```
事件时间- 事件或数据记录发生的时间点，即最初“在源头”创建的时间点。示例：如果事件是由汽车中的 GPS 传感器报告的地理位置变化，则关联的事件时间将是 GPS 传感器捕获位置变化的时间。
处理时间——事件或数据记录恰好被流处理应用程序处理的时间点，即记录被消费的时间点。处理时间可能比原始事件时间晚几毫秒、几小时或几天等。示例：想象一个分析应用程序读取和处理从汽车传感器报告的地理位置数据，以将其呈现给车队管理仪表板。在这里，分析应用程序中的处理时间可能是事件时间之后的毫秒或秒（例如，对于基于 Apache Kafka 和 Kafka Streams 的实时管道）或小时（例如，对于基于 Apache Hadoop 或 Apache Spark 的批处理管道）。
摄取时间- 事件或数据记录被 Kafka 代理存储在主题分区中的时间点。与事件时间的不同之处在于，此摄取时间戳是在 Kafka 代理将记录附加到目标主题时生成的，而不是在“在源”创建记录时生成的。处理时间的区别在于处理时间是流处理应用程序处理记录的时间。例如，如果一条记录从未被处理过，那么它就没有处理时间的概念，但它仍然有一个摄取时间。
```

````
Kafka Streams通过接口为每条数据记录分配一个时间戳TimestampExtractor
每一条消息又是有一个嵌入到消息内的时间戳，用来区分是否进入窗口，可以是事件时间也可以是摄取时间，取决于kafka streas的配置，kafka Streams是通过TimestampExtractor为kafka嵌入时间戳的
````

### 消息时间戳的传递性：

````

当通过处理某些输入记录生成新的输出记录时，例如context.forward()在process()函数调用中触发，输出记录时间戳直接继承自输入记录时间戳。
当通过周期函数（如 ）生成新的输出记录时Punctuator#punctuate()，输出记录时间戳定义为context.timestamp()流任务的当前内部时间（通过 获得）。
对于聚合，结果更新记录的时间戳将是对结果有贡献的所有输入记录的最大时间戳。
对于具有左输入记录和右输入记录的连接（流-流、表-表），分配输出记录的时间戳 max(left.ts, right.ts)。
对于流表连接，输出记录被分配来自流记录的时间戳。
对于聚合，Kafka Streams 还计算max 所有记录的时间戳，每个键，全局（对于非窗口）或每个窗口。
对于无状态操作，传递输入记录时间戳。对于flatMap发出多条记录的 和 兄弟，所有输出记录都继承相应输入记录的时间戳。
````

kafkaSteams如何保证处理消息：

```
请注意，Kafka Streams 端到端精确一次保证与其他流处理框架声称的保证之间的主要区别在于，Kafka Streams 与底层 Kafka 存储系统紧密集成，并确保提交输入主题偏移量、更新状态存储和写入输出主题将以原子方式完成，而不是将 Kafka 视为可能有副作用的外部系统。
```

乱序处理:

```
处理保证每条记录只被处理一次外，还要保证数据不要乱序
如果用户想要处理这种无序的数据，通常他们需要让他们的应用程序等待更长的时间，同时在等待时间内记录他们的状态，即在延迟、成本和正确性之间做出权衡决策。具体来说，在 Kafka Streams 中，用户可以为窗口化聚合配置他们的窗口操作符来实现这种权衡（详细信息可以在开发人员指南中找到). 至于 Join，用户必须意识到一些乱序数据还不能通过增加 Streams 中的延迟和成本来处理：
```



流和任务：

```
应用程序的处理器拓扑通过将其分解为多个任务来扩展。更具体地说，Kafka Streams 根据应用程序的输入流分区创建固定数量的任务，每个任务分配一个来自输入流的分区列表（即 Kafka 主题）
分配给任务的分区永远不会改变，因此每个任务都是应用程序并行度的固定单元。然后，任务可以根据分配的分区实例化自己的处理器拓扑；它们还为其分配的每个分区维护一个缓冲区，并一次处理来自这些记录缓冲区的消息。因此，流任务可以独立并行处理，无需人工干预。
```



```
主题任务和分区的分配器使用默认的StreamsPartitionAssignor ：kafkaStreams不允许我们更换


每个kafkastreams的实例中的线程数量可以配置，每个线程内运行的任务数量可以是一个或者多个
每个任务处理一个分区，并且每个任务都为自己的分区维护一个缓冲区，并依次来处理这些缓冲区中的消息

```

本地存储：

```
Kafka Streams 应用程序中的每个流任务都可以嵌入一个或多个可以通过 API 访问的本地状态存储，以存储和查询处理所需的数据。Kafka Streams 为此类本地状态存储提供容错和自动恢复。
每个任务都维护一个本地存储，通过本地存储保存状态，提供容错和自动恢复
```

容错：

```
Kafka Streams 中的任务利用 Kafka 消费者客户端提供的容错功能来处理故障。如果任务在失败的机器上运行，Kafka Streams 会自动在应用程序的剩余运行实例之一中重新启动任务。

对于每个状态存储，它维护一个复制的变更日志 Kafka 主题，在其中跟踪任何状态更新。这些变更日志主题也被分区，以便每个本地状态存储实例以及访问存储的任务都有自己专用的变更日志主题分区

如果任务在发生故障的机器上运行并在另一台机器上重新启动，Kafka Streams 保证通过在恢复处理新启动的任务之前重放相应的变更日志主题，将其关联的状态存储恢复到故障前的内容

请注意，任务（重新）初始化的成本通常主要取决于通过重放状态存储的相关变更日志主题来恢复状态的时间。为了最大限度地减少恢复时间，用户可以将他们的应用程序配置为具有本地状态的备用副本（即状态的完全复制副本），Kafka Streams 会将任务分配给已经存在此类备用副本的应用程序实例，以最小化任务（重新）初始化成本

RocksDB
本地状态存储，用来存储本地的状态，kafakStreams维护一个变更日志主题， 当某个任务失败的时候，可以通过变更日志主题来恢复，增加容错性
```



kafka streams DSL： 高级 API，提供最常见的数据转换操作，例如`map`、`filter`、`join`和`aggregations`开箱即用



配置问题：

```
（必需）应用程序 ID。每个流处理应用程序必须有一个唯一的 ID。必须为应用程序的所有实例提供相同的 ID。
更新应用程序时，application.id除非您想重用内部主题和状态存储中的现有数据，否则应该更改
```



![img](https://kafka.apache.org/34/images/streams-stateful_operations.png)

```
连接的前提：

输入主题的分区数量相同，同时输入主题的分区策略也要相同


```

```
TRADINGPRO-TRADER
```



```
http://172.16.46.44:36666/_manage/refresh
curl -X POST http://172.16.46.44:36666/_manage/refresh
```





